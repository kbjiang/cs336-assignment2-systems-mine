{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "# reshape to 2D, but block size is still seq_\n",
    "@triton.jit\n",
    "def flash_fwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    # program indices\n",
    "    query_tile_index = tl.program_id(0)\n",
    "    batch_index = tl.program_id(1)\n",
    "\n",
    "    # the shape is still seq_len*d_model\n",
    "    # different seq in a batch is located by `stride_qb`\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape=(N_QUERIES, ),\n",
    "        strides=(stride_lq,),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, ),\n",
    "        block_shape=(Q_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    O_block = tl.zeros((Q_TILE_SIZE, D), dtype=tl.float32)\n",
    "    L_block = tl.zeros((Q_TILE_SIZE, ), dtype=tl.float32)\n",
    "    \n",
    "    m = tl.full((Q_TILE_SIZE,), float('-inf'), dtype=tl.float32)\n",
    "    Q_block = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "\n",
    "    # Create offset indices for queries for causal masking\n",
    "    q_indices = query_tile_index * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)\n",
    "    for i in range(tl.cdiv(N_KEYS, K_TILE_SIZE)):\n",
    "        K_block = tl.load(K_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "        V_block = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "\n",
    "        # `allow_tf32=False` is important\n",
    "        # https://github.com/triton-lang/triton/issues/1840\n",
    "        S = scale * tl.dot(Q_block, tl.trans(K_block), allow_tf32=False)\n",
    "\n",
    "        if is_causal:\n",
    "            # nice trick to get a triangular mask\n",
    "            k_indices = i * K_TILE_SIZE + tl.arange(0, K_TILE_SIZE)\n",
    "            causal_mask = k_indices[None, :] <= q_indices[:, None]\n",
    "            S = tl.where(causal_mask, S, -1e6)\n",
    "            # tl.device_print(\"S\", S)\n",
    "\n",
    "        m_curr = tl.maximum(m, tl.max(S, axis=-1))\n",
    "\n",
    "        P = tl.exp(S - m_curr.expand_dims(axis=-1))\n",
    "\n",
    "        alpha = tl.exp(m - m_curr)\n",
    "        L_block = alpha * L_block  + tl.sum(P, axis=-1)\n",
    "\n",
    "        # according to Claude, tl does not have `diag` so need to use broadcasting\n",
    "        O_block = alpha[:, None] * O_block\n",
    "        # using `acc` for `tl.float32`\n",
    "        O_block = tl.dot(P.to(V_block.dtype), V_block, acc=O_block, allow_tf32=False)\n",
    "        m = m_curr\n",
    "\n",
    "        # Move the pointer to next tile\n",
    "        K_block_ptr = K_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "\n",
    "    O_block = (1 / L_block)[:, None] * O_block\n",
    "    L_block = m + tl.log(L_block)\n",
    "\n",
    "    tl.store(O_block_ptr, O_block, boundary_check=(0,))\n",
    "    tl.store(L_block_ptr, L_block, boundary_check=(0,))\n",
    "\n",
    "class FlashAttentionAutogradFunctionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        # cache Q, K and V?\n",
    "        batch_size, n_queries, D = Q.shape\n",
    "        _, n_keys, _ = K.shape\n",
    "\n",
    "        # reshape input tensor to 2D, i.e., remove batch dim\n",
    "        Q_input_shape = Q.shape\n",
    "        Q = rearrange(Q, \"... d -> (...) d\")\n",
    "        K_input_shape = K.shape\n",
    "        K = rearrange(K, \"... d -> (...) d\")\n",
    "        V = rearrange(V, \"... d -> (...) d\")\n",
    "\n",
    "        ctx.is_causal = is_causal\n",
    "\n",
    "        for t in [Q, K, V]:\n",
    "            assert t.is_cuda, \"Expected CUDA tensors\"\n",
    "            assert t.is_contiguous(), \"Our pointer arithmetic will assume contiguous inputs\"\n",
    "\n",
    "        ctx.Q_TILE_SIZE = 16\n",
    "        ctx.K_TILE_SIZE = 16\n",
    "        ctx.Q_input_shape = Q_input_shape\n",
    "        ctx.K_input_shape = K_input_shape\n",
    "\n",
    "        O = torch.empty(Q.shape, device=Q.device)\n",
    "        L = torch.zeros(batch_size * n_queries, device=Q.device)\n",
    "\n",
    "        stride_qb = n_queries * D\n",
    "        stride_qq = D\n",
    "        stride_qd = 1\n",
    "        stride_kb = n_keys * D\n",
    "        stride_kk = D\n",
    "        stride_kd = 1\n",
    "        stride_vb = n_keys * D\n",
    "        stride_vk = D\n",
    "        stride_vd = 1\n",
    "        stride_ob = stride_qb\n",
    "        stride_oq = stride_qq\n",
    "        stride_od = 1\n",
    "        stride_lb = n_queries\n",
    "        stride_lq = 1\n",
    "        scale = 1 / (D ** 0.5)\n",
    "         \n",
    "        # Your launch grid should be set as (Tq,batch_size), meaning each Triton program instance \n",
    "        # will load only elements from a single batch index, i.e., one seq_len * d_model,\n",
    "        # and only read/write to a single query tile of Q, O, and L.\n",
    "        flash_fwd_kernel[(math.ceil(n_queries/ctx.Q_TILE_SIZE), batch_size)](\n",
    "            Q, K, V, O, L,\n",
    "            stride_qb, stride_qq, stride_qd,\n",
    "            stride_kb, stride_kk, stride_kd,\n",
    "            stride_vb, stride_vk, stride_vd,\n",
    "            stride_ob, stride_oq, stride_od,\n",
    "            stride_lb, stride_lq,\n",
    "            n_queries, n_keys,\n",
    "            scale, D,\n",
    "            ctx.Q_TILE_SIZE,\n",
    "            ctx.K_TILE_SIZE,\n",
    "            is_causal,\n",
    "        )\n",
    "\n",
    "        O = O.view(Q_input_shape).contiguous()\n",
    "        Q = Q.view(Q_input_shape).contiguous()\n",
    "        K = K.view(K_input_shape).contiguous()\n",
    "        V = V.view(K_input_shape).contiguous()\n",
    "        L = L.view(batch_size, n_queries).contiguous()\n",
    "        ctx.save_for_backward(Q, K, V, L, O)\n",
    "        return O\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, L, O = ctx.saved_tensors\n",
    "        batch_size, _, D = Q.shape\n",
    "        DD = torch.sum(O * dO, axis=-1)\n",
    "        S = einsum(Q, K, \"... q d, ... k d -> ... q k\") / D ** 0.5\n",
    "        P = torch.exp(S - L[:, :, None])\n",
    "        dV = einsum(P, dO, \"... q k, ... q d -> ... k d\")\n",
    "        dP = einsum(dO, V, \"... q d, ... k d -> ... q k\")\n",
    "        dS = P * (dP - DD[:, :, None])\n",
    "        dQ = einsum(dS, K, \"... q k, ... k d -> ... q d\") / D ** 0.5\n",
    "        dK = einsum(dS, Q, \"... q k, ... q d -> ... k d\") / D ** 0.5\n",
    "        return dQ, dK, dV, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9952cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_attention import flash_backward_results, _attention_and_lse, _make_attn_inputs\n",
    "# from cs336_systems.flashattention_autograd_function import FlashAttentionAutogradFunctionPytorch\n",
    "\n",
    "device=\"cuda\"\n",
    "is_causal = False\n",
    "# impl = FlashAttentionAutogradFunctionTriton.apply\n",
    "q, k, v, do = _make_attn_inputs(device=device)\n",
    "o_ref, l_ref = _attention_and_lse(q, k, v, is_causal)\n",
    "o_ref.backward(do)\n",
    "dq_e, dk_e, dv_e = q.grad, k.grad, v.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa204b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_attention import flash_backward_results, _attention_and_lse, _make_attn_inputs\n",
    "device = \"cuda\"\n",
    "q, k, v, do = _make_attn_inputs(device)\n",
    "o = FlashAttentionAutogradFunctionTriton.apply(q, k, v, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_out = do\n",
    "Q, K, V, L, O = o.grad_fn.saved_tensors\n",
    "batch_size, _, D = Q.shape\n",
    "DD = torch.sum(O * grad_out, axis=-1)\n",
    "S = einsum(Q, K, \"... q d, ... k d -> ... q k\") / D ** 0.5\n",
    "P = torch.exp(S - L[:, :, None])\n",
    "dV = einsum(P, grad_out, \"... q k, ... q d -> ... k d\")\n",
    "dP = einsum(grad_out, V, \"... q d, ... k d -> ... q k\")\n",
    "dS = P * (dP - DD[:, :, None])\n",
    "dQ = einsum(dS, K, \"... q k, ... k d -> ... q d\") / D ** 0.5\n",
    "dK = einsum(dS, Q, \"... q k, ... q d -> ... k d\") / D ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
