{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350eb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_attention import _attention_and_lse, _make_attn_inputs\n",
    "from cs336_systems.flashattention_autograd_function_pytorch import FlashAttentionAutogradFunctionPytorch\n",
    "impl = FlashAttentionAutogradFunctionPytorch.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312091d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "is_causal = False\n",
    "q, k, v, _do = _make_attn_inputs(device)\n",
    "o = impl(q, k, v, is_causal)\n",
    "# Q, K, V, _do = _make_attn_inputs(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "@triton.jit\n",
    "def flash_fwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stide_lq,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "):\n",
    "    # program indices\n",
    "    query_tile_index = tl.program_id(0)\n",
    "    batch_index = tl.program_id(1)\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape=(N_QUERIES, ),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, ),\n",
    "        block_shape=(Q_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    O_block = tl.zeros(Q_TILE_SIZE, D, dtype=tl.float32)\n",
    "    L_block = tl.zeros(Q_TILE_SIZE, dtype=tl.float32)\n",
    "    m = tl.full((Q_TILE_SIZE,), float('-inf'), dtype=tl.float32)\n",
    "    Q_block = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "\n",
    "    for i in range(tl.cdiv(N_KEYS, K_TILE_SIZE)):\n",
    "        K_block = tl.load(K_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "        V_block = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "        S = scale * tl.dot(Q_block, tl.trans(K_block))\n",
    "        m_curr = tl.maximum(m, tl.max(S, axis=-1))\n",
    "        P = tl.exp(S - m_curr.expand_dims(axis=0))\n",
    "        L_block = tl.exp(m - m_curr) * L_block  + tl.sum(P, axis=-1)\n",
    "        # according to Claude, tl does not have `diag` so need to use broadcasting\n",
    "        O_block = tl.exp(m-m_curr)[:, None] * O_block + tl.dot(P, tl.trans(V_block))\n",
    "        m = m_curr\n",
    "\n",
    "        # Move the pointer to next tile\n",
    "        K_block_ptr = K_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "\n",
    "    O_block = (1 / L_block)[:, None] * O_block\n",
    "    L_block = m + tl.log(L_block)\n",
    "\n",
    "    tl.store(O_block_ptr, O_block, boundary_check=(0,))\n",
    "\n",
    "class FlashAttentionAutogradFunctionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        # cache Q, K and V?\n",
    "        D = Q.shape[-1]\n",
    "\n",
    "        # reshape input tensor to 2D, i.e., remove batch dim\n",
    "        Q_input_shape = Q.shape\n",
    "        Q = rearrange(Q, \"... d -> (...) d\")\n",
    "        K_input_shape = K.shape\n",
    "        K = rearrange(K, \"... d -> (...) d\")\n",
    "        V = rearrange(V, \"... d -> (...) d\")\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V)\n",
    "\n",
    "        for t in [Q, K, V]:\n",
    "            assert t.is_cuda, \"Expected CUDA tensors\"\n",
    "            assert t.is_contiguous(), \"Our pointer arithmetic will assume contiguous inputs\"\n",
    "\n",
    "        ctx.Q_TILE_SIZE = 16\n",
    "        ctx.K_TILE_SIZE = 16\n",
    "        ctx.Q_input_shape = Q_input_shape\n",
    "        ctx.K_input_shape = K_input_shape\n",
    "\n",
    "        O = torch.empty(Q.shape, device=Q.device)\n",
    "        L = torch.zeros(Q.shape[:-1], device=Q.device)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "@triton.jit\n",
    "def test_kernel():\n",
    "    x = tl.full([10], float('-inf'))\n",
    "    return x\n",
    "\n",
    "# But even then, you can't call it directly - you'd need to launch it as a kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e0571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ab1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton.language as tl \n",
    "\n",
    "tl.full([10], float('-inf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
