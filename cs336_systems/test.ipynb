{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "999b1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import einsum\n",
    "\n",
    "\n",
    "class FlashAttentionPytorch(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        O = torch.zeros(N_q, d, device='cuda')\n",
    "        L = torch.zeros((N_q, ), device='cuda')\n",
    "        for i in range(0, N_q, B_q):\n",
    "            Q_i = Q[i:i+B_q, :]\n",
    "            O_i = torch.zeros_like(Q_i)\n",
    "            l_i = torch.zeros(B_q, device='cuda')\n",
    "            m_i = torch.full((B_q,), float('-inf'), device='cuda')\n",
    "            for j in range(0, N_k, B_k):\n",
    "                K_j = K[j:j+B_k, :]\n",
    "                V_j = V[j:j+B_k, :]\n",
    "                S_j = einsum(Q_i, K_j, 'B_q d, B_k d -> B_q B_k') / (d**0.5)\n",
    "                assert S_j.shape == (B_q, B_k)\n",
    "\n",
    "                m_curr = torch.max(torch.cat([S_j, m_i[:, None]], axis=-1), axis=-1).values\n",
    "                print(m_curr)\n",
    "                print(S_j)\n",
    "                P_i = torch.exp(S_j - m_curr[:, None])\n",
    "                assert P_i.shape == (B_q, B_k)\n",
    "\n",
    "                l_i = torch.exp(m_i - m_curr)*l_i + torch.sum(P_i, axis=-1)\n",
    "                \n",
    "                _ = torch.diag(torch.exp(m_i - m_curr))\n",
    "                O_i = einsum(_, O_i, 'B_q B_q, B_q d -> B_q d') + einsum(P_i, V_j, 'B_q B_k, B_k d -> B_q d')\n",
    "                assert O_i.shape == (B_q, d)\n",
    "\n",
    "                m_i = m_curr\n",
    "            O_i = einsum(torch.diag(1 / l_i), O_i, 'B_q B_q, B_q d -> B_q d')\n",
    "            L_i = m_i + torch.log(l_i)\n",
    "\n",
    "            O[i:i+B_q] += O_i\n",
    "            L[i:i+B_q] += L_i\n",
    "        ctx.save_for_backward(Q, K, V, L, O)\n",
    "        return O, L\n",
    "    def backward(ctx):\n",
    "        raise NotImplementedError\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb61170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import einsum\n",
    "class FlashAttentionAutogradFunctionPytorch(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        O = torch.zeros(N_q, d)\n",
    "        L = torch.zeros(N_q, )\n",
    "        for i in range(0, N_q, B_q):\n",
    "            Q_i = Q[i:i+B_q, :]\n",
    "            O_i = torch.zeros_like(Q_i)\n",
    "            l_i = torch.zeros(B_q)\n",
    "            m_i = torch.full((B_q,), float('-inf'))\n",
    "            for j in range(0, N_k, B_k):\n",
    "                K_j = K[j:j+B_k, :]\n",
    "                V_j = V[j:j+B_k, :]\n",
    "                S_j = torch.rsqrt(torch.tensor(d)) * einsum(Q_i, K_j, 'B_q d, B_k d -> B_q B_k')\n",
    "                assert S_j.shape == (B_q, B_k)\n",
    "\n",
    "                m_curr = torch.max(torch.cat([m_i[:, None], S_j], axis=-1), axis=-1).values\n",
    "                P_i = torch.exp(S_j - m_curr[:, None])\n",
    "                assert P_i.shape == (B_q, B_k)\n",
    "\n",
    "                l_i = torch.exp(m_i - m_curr)*l_i + torch.sum(P_i, axis=-1)\n",
    "                \n",
    "                _ = torch.diag(torch.exp(m_i - m_curr))\n",
    "                O_i = einsum(_, O_i, 'B_q B_q, B_q d -> B_q d') + einsum(P_i, V_j, 'B_q B_k, B_k d -> B_q d')\n",
    "                assert O_i.shape == (B_q, d)\n",
    "\n",
    "                m_i = m_curr\n",
    "            O_i = einsum(torch.diag(1 / l_i), O_i, 'B_q B_q, B_q d -> B_q d')\n",
    "            L_i = m_i + torch.log(l_i)\n",
    "\n",
    "            O[i:i+B_q] += O_i\n",
    "            L[i:i+B_q] += L_i\n",
    "        ctx.save_for_backward(Q, K, V, L, O)\n",
    "        return O, L\n",
    "    def backward(ctx):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350eb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_attention import _attention_and_lse, _make_attn_inputs\n",
    "impl = FlashAttentionAutogradFunctionPytorch.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312091d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q, k, v, _do = _make_attn_inputs(device)\n",
    "o = impl(q, k, v, is_causal=False)\n",
    "\n",
    "# Extract L from the saved tensors\n",
    "assert o.grad_fn.saved_tensors is not None, \"No saved tensors found in the output tensor. Make sure your autograd forward is saving them using ctx.save_for_backward.\"\n",
    "maybe_ls = [t for t in o.grad_fn.saved_tensors if t.shape == (q.shape[0], q.shape[1])]\n",
    "\n",
    "assert len(maybe_ls) == 1, f\"Expected one tensor of shape {q.shape[0], q.shape[1]} in saved tensors, but found {len(maybe_ls)}. The tests require you to save exactly one tensor of this shape, corresponding to the log-sum-exp of the attention scores.\"\n",
    "l = maybe_ls[0]\n",
    "\n",
    "o_ref, l_ref = _attention_and_lse(q, k, v, is_causal)\n",
    "\n",
    "torch.testing.assert_close(o, o_ref, rtol=1e-2, atol=1e-2)\n",
    "torch.testing.assert_close(l, l_ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import timeit\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "def f(x, repeat):\n",
    "    for _ in range(repeat):\n",
    "        x = x * 2\n",
    "    return x\n",
    "\n",
    "# Compile the function for better performance\n",
    "f_compiled = torch.compile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b88784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f_func, repeat, iters, sz=2**24, name=\"Function\"):\n",
    "    input_tensor = torch.randn(sz, device=\"cuda\")\n",
    "    \n",
    "    # Warmup for compiled functions\n",
    "    for _ in range(3):\n",
    "        _ = f_func(input_tensor.clone(), repeat)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    time = timeit.timeit(lambda: f_func(input_tensor.clone(), repeat), number=iters)\n",
    "    \n",
    "    # For compiled functions, PyTorch may optimize memory access patterns\n",
    "    # so we should measure actual memory bandwidth rather than theoretical\n",
    "    flop = sz * repeat * iters\n",
    "    \n",
    "    # Conservative estimate: at minimum we need to read input once and write output once\n",
    "    # But compiled version might fuse operations and reduce intermediate memory accesses\n",
    "    memory_conservative = 4 * 2 * sz * iters  # Just input read + final write per benchmark iteration\n",
    "    \n",
    "    flops = flop / time\n",
    "    mem_bd_conservative = memory_conservative / time\n",
    "    \n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Size: {sz:,} elements ({sz*4/1e6:.1f} MB)\")\n",
    "    print(f\"Repeat: {repeat}, Iterations: {iters}\")\n",
    "    print(f\"Time: {time:.4f} seconds\")\n",
    "    print(f\"FLOPS: {flops/1e9:.2f} GFLOPS\")\n",
    "    print(f\"Memory BW (conservative): {mem_bd_conservative/1e9:.2f} GB/s\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"time\": time, \n",
    "        \"flops\": flops, \n",
    "        \"memory_bandwidth_conservative\": mem_bd_conservative,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep across different repeat values (powers of 2)\n",
    "repeat_values = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
    "iters = 10\n",
    "sz = 2**22  # Smaller size for faster sweep\n",
    "\n",
    "print(\"Running sweep across repeat values:\\n\")\n",
    "\n",
    "# RTX 3060 specs for comparison\n",
    "RTX_3060_PEAK_FLOPS = 13e12  # 13 TFLOPS\n",
    "RTX_3060_MEMORY_BW = 360e9   # 360 GB/s\n",
    "\n",
    "results = []\n",
    "for repeat in repeat_values:\n",
    "    print(f\"Testing repeat = {repeat}\")\n",
    "    result = benchmark(f_compiled, repeat, iters, sz, f\"Repeat-{repeat}\")\n",
    "    \n",
    "    # Calculate utilization percentages\n",
    "    flops_util = (result['flops'] / RTX_3060_PEAK_FLOPS) * 100\n",
    "    mem_util = (result['memory_bandwidth_conservative'] / RTX_3060_MEMORY_BW) * 100\n",
    "    \n",
    "    print(f\"  FLOPS Utilization: {flops_util:.2f}%\")\n",
    "    print(f\"  Memory BW Utilization: {mem_util:.2f}%\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results.append({\n",
    "        'repeat': repeat,\n",
    "        'time': result['time'],\n",
    "        'flops': result['flops'],\n",
    "        'memory_bw': result['memory_bandwidth_conservative'],\n",
    "        'flops_util': flops_util,\n",
    "        'mem_util': mem_util\n",
    "    })\n",
    "\n",
    "print(f\"\\nSummary of {len(results)} experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "repeats = [r['repeat'] for r in results]\n",
    "times = [r['time'] for r in results]\n",
    "flops = [r['flops']/1e9 for r in results]  # Convert to GFLOPS\n",
    "mem_bw = [r['memory_bw']/1e9 for r in results]  # Convert to GB/s\n",
    "flops_util = [r['flops_util'] for r in results]\n",
    "\n",
    "# Runtime vs Repeat (log base 2 x-axis)\n",
    "ax1.plot(repeats, times, 'b-o', linewidth=2)\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlabel('Number of Repeats')\n",
    "ax1.set_ylabel('Runtime (seconds)')\n",
    "ax1.set_title('Runtime vs Repeat Count')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(repeats)\n",
    "ax1.set_xticklabels(repeats)\n",
    "\n",
    "# FLOPS vs Repeat (log base 2 x-axis)\n",
    "ax2.plot(repeats, flops, 'g-o', linewidth=2)\n",
    "ax2.set_xscale('log', base=2)\n",
    "ax2.axhline(y=13000, color='r', linestyle='--', label='RTX 3060 Peak (13 TFLOPS)')\n",
    "ax2.set_xlabel('Number of Repeats')\n",
    "ax2.set_ylabel('GFLOPS')\n",
    "ax2.set_title('FLOPS vs Repeat Count')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(repeats)\n",
    "ax2.set_xticklabels(repeats)\n",
    "\n",
    "# Memory Bandwidth vs Repeat (log base 2 x-axis)\n",
    "ax3.plot(repeats, mem_bw, 'purple', marker='o', linewidth=2)\n",
    "ax3.set_xscale('log', base=2)\n",
    "ax3.axhline(y=360, color='r', linestyle='--', label='RTX 3060 Peak (360 GB/s)')\n",
    "ax3.set_xlabel('Number of Repeats')\n",
    "ax3.set_ylabel('Memory Bandwidth (GB/s)')\n",
    "ax3.set_title('Memory Bandwidth vs Repeat Count')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xticks(repeats)\n",
    "ax3.set_xticklabels(repeats)\n",
    "\n",
    "# Utilization percentages (log base 2 x-axis)\n",
    "ax4.plot(repeats, flops_util, 'g-o', label='FLOPS Utilization', linewidth=2)\n",
    "ax4.plot(repeats, [r['mem_util'] for r in results], 'purple', marker='s', label='Memory BW Utilization', linewidth=2)\n",
    "ax4.set_xscale('log', base=2)\n",
    "ax4.set_xlabel('Number of Repeats')\n",
    "ax4.set_ylabel('Utilization (%)')\n",
    "ax4.set_title('Hardware Utilization vs Repeat Count')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xticks(repeats)\n",
    "ax4.set_xticklabels(repeats)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best performers\n",
    "best_flops = max(results, key=lambda x: x['flops_util'])\n",
    "best_memory = max(results, key=lambda x: x['mem_util'])\n",
    "\n",
    "print(f\"Best FLOPS utilization: {best_flops['flops_util']:.2f}% at repeat={best_flops['repeat']}\")\n",
    "print(f\"Best Memory utilization: {best_memory['mem_util']:.2f}% at repeat={best_memory['repeat']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
