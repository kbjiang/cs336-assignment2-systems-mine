{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfe2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"pytorch_attention_results.jsonl\", lines=True)\n",
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb2d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "128*10000*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debe5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "8*128*16384*4*10000 / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "524288/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8066b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cs336_basics\n",
    "from cs336_basics.model import BasicsTransformerLM\n",
    "from cs336_basics.data import get_batch\n",
    "from cs336_basics.optimizer import AdamW\n",
    "from cs336_basics.nn_utils import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e11507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Bool, Int\n",
    "from einops import rearrange, einsum\n",
    "from cs336_basics.nn_utils import softmax, cross_entropy\n",
    "from cs336_basics.model import Linear\n",
    "from cs336_basics.optimizer import AdamW\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    Q: Float[Tensor, \" ... queries d_k\"],\n",
    "    K: Float[Tensor, \" ... keys    d_k\"],\n",
    "    V: Float[Tensor, \" ... keys    d_v\"],\n",
    "    mask: Bool[Tensor, \" ... queries keys\"] | None = None,\n",
    ") -> Float[Tensor, \" ... queries d_v\"]:\n",
    "    \"\"\"Scaled dot-product attention.\n",
    "\n",
    "    This function implements Eq. 1 of the Transformer paper.\n",
    "\n",
    "    Args:\n",
    "        Q: Tensor of queries, may have any number of leading dimensions.\n",
    "        K: Tensor of keys, sharing leading dimensions with Q.\n",
    "        V: Tensor of values, sharding leading dimensions with Q and K.\n",
    "        mask: An (optional) mask of shape (..., seq_len, seq_len).\n",
    "            Attention scores for positions with a mask value of `False` should\n",
    "            be masked out, i.e., not affect the softmaxed attention probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor of shape (..., seq_len, value_dimension)\n",
    "        with the output of running your scaled dot product attention\n",
    "        implementation with the provided key, query, and value tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    d_k = K.shape[-1]\n",
    "    attention_scores = einsum(Q, K, \"... query d_k, ... key d_k -> ... query key\") / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        attention_scores = torch.where(mask, attention_scores, float(\"-inf\"))\n",
    "\n",
    "    attention_weights = softmax(attention_scores, dim=-1)  # Softmax over the key dimension\n",
    "\n",
    "    return einsum(attention_weights, V, \"... query key, ... key d_v ->  ... query d_v\")\n",
    "\n",
    "\n",
    "class AttentionWithLinear(nn.Module):\n",
    "    \"\"\"Single-Head Self-Attention\n",
    "    Args:\n",
    "        d_model: head embedding size\n",
    "        vocab_size: for LM output\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape `(batch_size, sequence_length, vocab_size)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        vocab_size,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lm_head = Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, Q, K, V) -> Float[Tensor, \" ... seq d_v\"]:\n",
    "        seq_len = Q.shape[-2]  # Get actual sequence length from input\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=Q.device)).bool()\n",
    "        \n",
    "        # Shape: (..., num_heads, sequence_length, d_k)\n",
    "        attn_output = scaled_dot_product_attention(K=K, Q=Q, V=V, mask=causal_mask)\n",
    "\n",
    "        # Shape: (..., num_heads, sequence_length, vocab_size)\n",
    "        output = self.lm_head(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_len = 4096\n",
    "d_model = 128\n",
    "vocab_size = 10000\n",
    "device = \"cuda\"\n",
    "Q = torch.randn((batch_size, seq_len, d_model), device=device)\n",
    "K = torch.randn((batch_size, seq_len, d_model), device=device)\n",
    "V = torch.randn((batch_size, seq_len, d_model), device=device)\n",
    "target = torch.randint(0, vocab_size, (batch_size, seq_len), device=Q.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b80b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionWithLinear(d_model, vocab_size).to(device)\n",
    "y_hat = model(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cross_entropy(y_hat, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 8\n",
    "d_models = [16, 32, 64, 128]\n",
    "seq_lens = [256, 1024, 4096, 8192, 16384]\n",
    "\n",
    "for d_model, seq_len in zip(d_models, seq_lens):\n",
    "    Q = torch.randn((BATCH_SIZE, seq_len, d_model)).to('cuda')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10_000\n",
    "ROPE_THETA = 10_000\n",
    "BATCH_SIZE = 4\n",
    "CONTEXT_LENGTH = 256\n",
    "D_MODEL = 768\n",
    "D_FF = 3072\n",
    "NUM_LAYERS = 12\n",
    "NUM_HEADS = 12\n",
    "\n",
    "WARMUP_STEPS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = BasicsTransformerLM(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    context_length = CONTEXT_LENGTH,\n",
    "    d_model = D_MODEL,\n",
    "    num_layers = NUM_LAYERS,\n",
    "    num_heads = NUM_HEADS,\n",
    "    d_ff = D_FF,\n",
    "    rope_theta = ROPE_THETA,\n",
    ")\n",
    "model.to(\"cuda:0\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19195a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# np_file = \"../data/ts_valid.npy\"\n",
    "# dataset = np.load(np_file)\n",
    "dataset = np.random.randint(0, VOCAB_SIZE, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from functools import partial\n",
    "\n",
    "def train_step(do_backward=False):\n",
    "    x, y = get_batch(\n",
    "        dataset, BATCH_SIZE, CONTEXT_LENGTH, \"cuda\" \n",
    "    )\n",
    "    y_hat = model(x)\n",
    "    if do_backward:\n",
    "        optimizer.zero_grad()\n",
    "        loss = cross_entropy(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def run_test(warmup_steps, train_steps, do_backward):\n",
    "    for _ in range(warmup_steps):\n",
    "        train_step()\n",
    "\n",
    "    train_step_ = partial(train_step, do_backward=do_backward)\n",
    "    elapsed = timeit.timeit(train_step_, number=train_steps)\n",
    "    print(f\"Time for {train_steps} training step: {elapsed:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f177bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(5, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdea39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d31221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 10, bias=False)\n",
    "        self.ln = nn.LayerNorm(10)\n",
    "        self.fc2 = nn.Linear(10, out_features, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.ln(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "INPUT_SIZE = 3\n",
    "OUTPUT_SIZE = 4\n",
    "BATCH_SIZE = 4\n",
    "model = ToyModel(INPUT_SIZE, OUTPUT_SIZE).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "x = torch.randn(BATCH_SIZE, INPUT_SIZE).cuda()\n",
    "y = torch.randn(BATCH_SIZE, OUTPUT_SIZE).cuda()\n",
    "\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "with torch.autocast(device_type=\"cuda\", dtype=DTYPE):\n",
    "    print(f\"Input: {x.dtype}\")\n",
    "    x_after_fc1 = model.fc1(x)\n",
    "    print(f\"After fc1: {x_after_fc1.dtype}\")\n",
    "    x_after_ln = model.ln(x_after_fc1)\n",
    "    print(f\"After LN: {x_after_ln.dtype}\")\n",
    "    y_pred = model.relu(model.fc2(x_after_ln))\n",
    "    print(f\"Logits: {y_pred.dtype}\")\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    print(f\"Loss: {loss.dtype}\")\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    optimizer.zero_grad()\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.dtype)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"=\"*60)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.dtype, param.grad.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([1e-3], dtype=torch.float16)\n",
    "rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True))\n",
    "print(rms)\n",
    "x = torch.tensor([1e-3], dtype=torch.bfloat16)\n",
    "rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype, param.grad.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype, param.grad.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect('report-mixed.sqlite')\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT text, SUM(end - start) as total_ns\n",
    "    FROM NVTX_EVENTS \n",
    "    WHERE text = 'Backward Pass'\n",
    "    GROUP BY text\n",
    "\"\"\", conn)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema for a specific table (e.g., NVTX_EVENTS)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(NVTX_EVENTS);\")\n",
    "columns = cursor.fetchall()\n",
    "print(\"NVTX_EVENTS table schema:\")\n",
    "for col in columns:\n",
    "    print(f\"  {col[1]} ({col[2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60faafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv('benchmark_results.csv')\n",
    "\n",
    "# Define size order for proper x-axis ordering\n",
    "size_order = ['small', 'medium', 'large', 'xl', '2.7b']\n",
    "df['size'] = pd.Categorical(df['size'], categories=size_order, ordered=True)\n",
    "\n",
    "# Create the plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Forward Pass Plot\n",
    "for precision in ['full', 'mixed']:\n",
    "    data = df[df['precision'] == precision]\n",
    "    ax1.plot(data['size'], data['forward_time_seconds'], \n",
    "             marker='o', linewidth=2, label=f'{precision} precision')\n",
    "\n",
    "ax1.set_title('Forward Pass Time vs Model Size')\n",
    "ax1.set_xlabel('Model Size')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Backward Pass Plot\n",
    "for precision in ['full', 'mixed']:\n",
    "    data = df[df['precision'] == precision]\n",
    "    ax2.plot(data['size'], data['backward_time_seconds'], \n",
    "             marker='o', linewidth=2, label=f'{precision} precision')\n",
    "\n",
    "ax2.set_title('Backward Pass Time vs Model Size')\n",
    "ax2.set_xlabel('Model Size')\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('benchmark_timing_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print some analysis\n",
    "print(\"Forward Pass Speedup with Mixed Precision:\")\n",
    "for size in size_order:\n",
    "    full_time = df[(df['size'] == size) & (df['precision'] == 'full')]['forward_time_seconds'].iloc[0]\n",
    "    mixed_time = df[(df['size'] == size) & (df['precision'] == 'mixed')]['forward_time_seconds'].iloc[0]\n",
    "    speedup = full_time / mixed_time\n",
    "    print(f\"{size}: {speedup:.2f}x {'speedup' if speedup > 1 else 'slowdown'}\")\n",
    "\n",
    "print(\"\\nBackward Pass Speedup with Mixed Precision:\")\n",
    "for size in size_order:\n",
    "    full_time = df[(df['size'] == size) & (df['precision'] == 'full')]['backward_time_seconds'].iloc[0]\n",
    "    mixed_time = df[(df['size'] == size) & (df['precision'] == 'mixed')]['backward_time_seconds'].iloc[0]\n",
    "    speedup = full_time / mixed_time\n",
    "    print(f\"{size}: {speedup:.2f}x {'speedup' if speedup > 1 else 'slowdown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6702153e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "gpu_ok = False\n",
    "if torch.cuda.is_available():\n",
    "    device_cap = torch.cuda.get_device_capability()\n",
    "    if device_cap in ((7, 0), (8, 0), (9, 0)):\n",
    "        gpu_ok = True\n",
    "\n",
    "if not gpu_ok:\n",
    "    warnings.warn(\n",
    "        \"GPU is not NVIDIA V100, A100, or H100. Speedup numbers may be lower \"\n",
    "        \"than expected.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.randn(10, 100)\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.relu(self.lin(x))\n",
    "\n",
    "mod = MyModule()\n",
    "mod.compile()\n",
    "print(mod(t))\n",
    "# or:\n",
    "# opt_mod = torch.compile(mod)\n",
    "# print(opt_mod(t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
