{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350eb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_attention import _attention_and_lse, _make_attn_inputs\n",
    "from cs336_systems.flashattention_autograd_function_pytorch import FlashAttentionAutogradFunctionPytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "@triton.jit\n",
    "def flash_fwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "):\n",
    "    # program indices\n",
    "    query_tile_index = tl.program_id(0)\n",
    "    batch_index = tl.program_id(1)\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape=(N_QUERIES, ),\n",
    "        strides=(stride_lq,),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, ),\n",
    "        block_shape=(Q_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    O_block = tl.zeros((Q_TILE_SIZE, D), dtype=tl.float32)\n",
    "    L_block = tl.zeros((Q_TILE_SIZE, ), dtype=tl.float32)\n",
    "    m = tl.full((Q_TILE_SIZE,), float('-inf'), dtype=tl.float32)\n",
    "    Q_block = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "\n",
    "    for i in range(tl.cdiv(N_KEYS, K_TILE_SIZE)):\n",
    "        # K_block = tl.load(K_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "        # V_block = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "        K_block = tl.load(K_block_ptr)\n",
    "        V_block = tl.load(V_block_ptr)\n",
    "\n",
    "        S = scale * tl.dot(Q_block, tl.trans(K_block.to(tl.float32)))\n",
    "        # print\n",
    "        tl.device_print(\"S\", S)\n",
    "\n",
    "        m_curr = tl.maximum(m, tl.max(S, axis=-1))\n",
    "\n",
    "        P = tl.exp(S - m_curr.expand_dims(axis=-1))\n",
    "\n",
    "        alpha = tl.exp(m - m_curr)\n",
    "        L_block = alpha * L_block  + tl.sum(P, axis=-1)\n",
    "        # according to Claude, tl does not have `diag` so need to use broadcasting\n",
    "        O_block = alpha[:, None] * O_block\n",
    "        # using `acc` for `tl.float32`\n",
    "        O_block = tl.dot(P.to(V_block.dtype), V_block, acc=O_block)\n",
    "        m = m_curr\n",
    "\n",
    "        # Move the pointer to next tile\n",
    "        K_block_ptr = K_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "\n",
    "    O_block = (1 / L_block)[:, None] * O_block\n",
    "    L_block = m + tl.log(L_block)\n",
    "\n",
    "    tl.store(O_block_ptr, O_block, boundary_check=(0,))\n",
    "    tl.store(L_block_ptr, L_block, boundary_check=(0,))\n",
    "\n",
    "class FlashAttentionAutogradFunctionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        # cache Q, K and V?\n",
    "        batch_size, n_queries, D = Q.shape\n",
    "        _, n_keys, _ = K.shape\n",
    "\n",
    "        # reshape input tensor to 2D, i.e., remove batch dim\n",
    "        Q_input_shape = Q.shape\n",
    "        Q = rearrange(Q, \"... d -> (...) d\")\n",
    "        K_input_shape = K.shape\n",
    "        K = rearrange(K, \"... d -> (...) d\")\n",
    "        V = rearrange(V, \"... d -> (...) d\")\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V)\n",
    "\n",
    "        for t in [Q, K, V]:\n",
    "            assert t.is_cuda, \"Expected CUDA tensors\"\n",
    "            assert t.is_contiguous(), \"Our pointer arithmetic will assume contiguous inputs\"\n",
    "\n",
    "        ctx.Q_TILE_SIZE = 16\n",
    "        ctx.K_TILE_SIZE = 16\n",
    "        ctx.Q_input_shape = Q_input_shape\n",
    "        ctx.K_input_shape = K_input_shape\n",
    "\n",
    "        O = torch.empty(Q.shape, device=Q.device)\n",
    "        L = torch.zeros(Q.shape[0], device=Q.device)\n",
    "\n",
    "        stride_qb = n_queries * D\n",
    "        stride_qq = D\n",
    "        stride_qd = 1\n",
    "        stride_kb = n_keys * D\n",
    "        stride_kk = D\n",
    "        stride_kd = 1\n",
    "        stride_vb = n_keys * D\n",
    "        stride_vk = D\n",
    "        stride_vd = 1\n",
    "        stride_ob = stride_qb\n",
    "        stride_oq = stride_qq\n",
    "        stride_od = 1\n",
    "        stride_lb = Q_input_shape[0]\n",
    "        stride_lq = 1\n",
    "        scale = 1 / (D ** 0.5)\n",
    "         \n",
    "        # print(stride_qb, stride_qq, stride_qd)\n",
    "        # print(stride_kb, stride_kk, stride_kd)\n",
    "        # print(stride_vb, stride_vk, stride_vd)\n",
    "        # print(stride_ob, stride_oq, stride_od)\n",
    "        # print(N_QUERIES, N_KEYS)\n",
    "        flash_fwd_kernel[(math.ceil(O.shape[1]/ctx.Q_TILE_SIZE), Q_input_shape[0])](\n",
    "            Q, K, V, O, L,\n",
    "            stride_qb, stride_qq, stride_qd,\n",
    "            stride_kb, stride_kk, stride_kd,\n",
    "            stride_vb, stride_vk, stride_vd,\n",
    "            stride_ob, stride_oq, stride_od,\n",
    "            stride_lb, stride_lq,\n",
    "            n_queries, n_keys,\n",
    "            scale, D,\n",
    "            ctx.Q_TILE_SIZE,\n",
    "            ctx.K_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        return O.view(Q_input_shape)\n",
    "    def backward(ctx):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "is_causal = False\n",
    "impl = FlashAttentionAutogradFunctionTriton.apply\n",
    "\n",
    "def _make_attn_inputs(device=None):\n",
    "    torch.random.manual_seed(0)\n",
    "    batch_size = 4\n",
    "    n_queries = 128\n",
    "    n_keys = 128\n",
    "    D = 64\n",
    "    q = torch.randn(batch_size, n_queries, D, device=device, requires_grad=True)\n",
    "    k = torch.randn(batch_size, n_keys, D, device=device, requires_grad=True)\n",
    "    v = torch.randn(batch_size, n_keys, D, device=device, requires_grad=True)\n",
    "    do = torch.randn(batch_size, n_queries, D, device=device)\n",
    "\n",
    "    return q, k, v, do\n",
    "\n",
    "q, k, v, do = _make_attn_inputs(device)\n",
    "o_ref, l_ref = _attention_and_lse(q, k, v, is_causal)\n",
    "o = impl(q, k, v, is_causal)\n",
    "# Q, K, V, _do = _make_attn_inputs(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf371744",
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(Q, K, V, is_causal=False):\n",
    "    device = Q.device\n",
    "    batch_size, N_q, d = Q.shape\n",
    "    _, N_k, _ = K.shape\n",
    "    B_q = 16\n",
    "    B_k = 16\n",
    "    O = torch.zeros_like(Q, device=device)\n",
    "    L = torch.zeros(batch_size, N_q, device=device)\n",
    "    for i in range(0, N_q, B_q):\n",
    "        Q_i = Q[:, i:i+B_q, :]\n",
    "        O_i = torch.zeros_like(Q_i, device=device)\n",
    "        l_i = torch.zeros(batch_size, B_q, device=device)\n",
    "        m_i = torch.full((batch_size, B_q), float('-inf'), device=device)\n",
    "        for j in range(0, N_k, B_k):\n",
    "            K_j = K[:, j:j+B_k, :]\n",
    "            V_j = V[:, j:j+B_k, :]\n",
    "            S_j = 1 / d**(0.5) * einsum(Q_i, K_j, '... B_q d, ... B_k d -> ... B_q B_k')\n",
    "            assert S_j.shape == (batch_size, B_q, B_k)\n",
    "            print(S_j)\n",
    "            print(S_j.dtype)\n",
    "\n",
    "            m_curr = torch.max(torch.cat([m_i[:, :, None], S_j], axis=-1), axis=-1).values\n",
    "            P_i = torch.exp(S_j - m_curr[:, :, None])\n",
    "            assert P_i.shape == (batch_size, B_q, B_k)\n",
    "\n",
    "\n",
    "            l_i = torch.exp(m_i - m_curr)*l_i + torch.sum(P_i, axis=-1)\n",
    "            \n",
    "            _ = torch.diag_embed(torch.exp(m_i - m_curr))\n",
    "            O_i = einsum(_, O_i, '... B_q B_q, ... B_q d -> ... B_q d') + einsum(P_i, V_j, '... B_q B_k, ... B_k d -> ... B_q d')\n",
    "            assert O_i.shape == (batch_size, B_q, d)\n",
    "\n",
    "            m_i = m_curr\n",
    "        O_i = einsum(torch.diag_embed(1 / l_i), O_i, '... B_q B_q, ... B_q d -> ... B_q d')\n",
    "        L_i = m_i + torch.log(l_i)\n",
    "\n",
    "        O[:, i:i+B_q] += O_i\n",
    "        L[:, i:i+B_q] += L_i\n",
    "\n",
    "forward(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ddced",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_ref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
