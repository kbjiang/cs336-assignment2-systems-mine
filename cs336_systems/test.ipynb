{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from einops import einsum, rearrange\n",
    "\n",
    "@triton.jit\n",
    "def flash_fwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr, L_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D: tl.constexpr,\n",
    "    Q_TILE_SIZE: tl.constexpr,\n",
    "    K_TILE_SIZE: tl.constexpr,\n",
    "    is_causal: tl.constexpr,\n",
    "):\n",
    "    # program indices\n",
    "    query_tile_index = tl.program_id(0)\n",
    "    batch_index = tl.program_id(1)\n",
    "\n",
    "    # the shape is still seq_len*d_model\n",
    "    # different seq in a batch is located by `stride_qb`\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index * stride_qb,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_qq, stride_qd),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_kk, stride_kd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape=(N_KEYS, D),\n",
    "        strides=(stride_vk, stride_vd),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(K_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape=(N_QUERIES, D),\n",
    "        strides=(stride_oq, stride_od),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, 0),\n",
    "        block_shape=(Q_TILE_SIZE, D),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape=(N_QUERIES, ),\n",
    "        strides=(stride_lq,),\n",
    "        offsets=(query_tile_index * Q_TILE_SIZE, ),\n",
    "        block_shape=(Q_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    O_block = tl.zeros((Q_TILE_SIZE, D), dtype=tl.float32)\n",
    "    L_block = tl.zeros((Q_TILE_SIZE, ), dtype=tl.float32)\n",
    "    \n",
    "    m = tl.full((Q_TILE_SIZE,), float('-inf'), dtype=tl.float32)\n",
    "    Q_block = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "\n",
    "    # Create offset indices for queries for causal masking\n",
    "    q_indices = query_tile_index * Q_TILE_SIZE + tl.arange(0, Q_TILE_SIZE)\n",
    "    for i in range(tl.cdiv(N_KEYS, K_TILE_SIZE)):\n",
    "        K_block = tl.load(K_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "        V_block = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "\n",
    "        # `allow_tf32=False` is important\n",
    "        # https://github.com/triton-lang/triton/issues/1840\n",
    "        S = scale * tl.dot(Q_block, tl.trans(K_block), allow_tf32=False)\n",
    "\n",
    "        if is_causal:\n",
    "            k_indices = i * K_TILE_SIZE + tl.arange(0, K_TILE_SIZE)\n",
    "            causal_mask = k_indices[None, :] > q_indices[:, None]\n",
    "            S = tl.where(causal_mask, S, float('-inf'))\n",
    "\n",
    "        m_curr = tl.maximum(m, tl.max(S, axis=-1))\n",
    "\n",
    "        P = tl.exp(S - m_curr.expand_dims(axis=-1))\n",
    "\n",
    "        alpha = tl.exp(m - m_curr)\n",
    "        L_block = alpha * L_block  + tl.sum(P, axis=-1)\n",
    "\n",
    "        # according to Claude, tl does not have `diag` so need to use broadcasting\n",
    "        O_block = alpha[:, None] * O_block\n",
    "        # using `acc` for `tl.float32`\n",
    "        O_block = tl.dot(P.to(V_block.dtype), V_block, acc=O_block, allow_tf32=False)\n",
    "        m = m_curr\n",
    "\n",
    "        # Move the pointer to next tile\n",
    "        K_block_ptr = K_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "        V_block_ptr = V_block_ptr.advance((K_TILE_SIZE, 0))\n",
    "\n",
    "    O_block = (1 / L_block)[:, None] * O_block\n",
    "    L_block = m + tl.log(L_block)\n",
    "\n",
    "    tl.store(O_block_ptr, O_block, boundary_check=(0,))\n",
    "    tl.store(L_block_ptr, L_block, boundary_check=(0,))\n",
    "\n",
    "class FlashAttentionAutogradFunctionTriton(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal=False):\n",
    "        # cache Q, K and V?\n",
    "        batch_size, n_queries, D = Q.shape\n",
    "        _, n_keys, _ = K.shape\n",
    "\n",
    "        # reshape input tensor to 2D, i.e., remove batch dim\n",
    "        Q_input_shape = Q.shape\n",
    "        Q = rearrange(Q, \"... d -> (...) d\")\n",
    "        K_input_shape = K.shape\n",
    "        K = rearrange(K, \"... d -> (...) d\")\n",
    "        V = rearrange(V, \"... d -> (...) d\")\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V)\n",
    "        ctx.is_causal = is_causal\n",
    "\n",
    "        for t in [Q, K, V]:\n",
    "            assert t.is_cuda, \"Expected CUDA tensors\"\n",
    "            assert t.is_contiguous(), \"Our pointer arithmetic will assume contiguous inputs\"\n",
    "\n",
    "        ctx.Q_TILE_SIZE = 16\n",
    "        ctx.K_TILE_SIZE = 16\n",
    "        ctx.Q_input_shape = Q_input_shape\n",
    "        ctx.K_input_shape = K_input_shape\n",
    "\n",
    "        O = torch.empty(Q.shape, device=Q.device)\n",
    "        L = torch.zeros(batch_size * n_queries, device=Q.device)\n",
    "\n",
    "        stride_qb = n_queries * D\n",
    "        stride_qq = D\n",
    "        stride_qd = 1\n",
    "        stride_kb = n_keys * D\n",
    "        stride_kk = D\n",
    "        stride_kd = 1\n",
    "        stride_vb = n_keys * D\n",
    "        stride_vk = D\n",
    "        stride_vd = 1\n",
    "        stride_ob = stride_qb\n",
    "        stride_oq = stride_qq\n",
    "        stride_od = 1\n",
    "        stride_lb = n_queries\n",
    "        stride_lq = 1\n",
    "        scale = 1 / (D ** 0.5)\n",
    "         \n",
    "        # Your launch grid should be set as (Tq,batch_size), meaning each Triton program instance \n",
    "        # will load only elements from a single batch index, i.e., one seq_len * d_model,\n",
    "        # and only read/write to a single query tile of Q, O, and L.\n",
    "        flash_fwd_kernel[(math.ceil(n_queries/ctx.Q_TILE_SIZE), batch_size)](\n",
    "            Q, K, V, O, L,\n",
    "            stride_qb, stride_qq, stride_qd,\n",
    "            stride_kb, stride_kk, stride_kd,\n",
    "            stride_vb, stride_vk, stride_vd,\n",
    "            stride_ob, stride_oq, stride_od,\n",
    "            stride_lb, stride_lq,\n",
    "            n_queries, n_keys,\n",
    "            scale, D,\n",
    "            ctx.Q_TILE_SIZE,\n",
    "            ctx.K_TILE_SIZE,\n",
    "            is_causal,\n",
    "        )\n",
    "\n",
    "        O = O.view(Q_input_shape).contiguous()\n",
    "        L = L.view(batch_size, n_queries).contiguous()\n",
    "        ctx.save_for_backward(Q, K, V, L, O)\n",
    "        return O\n",
    "    def backward(ctx):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d79158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_attention import _attention_and_lse, _make_attn_inputs\n",
    "# from cs336_systems.flashattention_autograd_function import FlashAttentionAutogradFunctionPytorch\n",
    "\n",
    "device=\"cuda\"\n",
    "is_causal = True\n",
    "impl = FlashAttentionAutogradFunctionTriton.apply\n",
    "\n",
    "def _make_attn_inputs(device=None):\n",
    "    torch.random.manual_seed(0)\n",
    "    batch_size = 1\n",
    "    n_queries = 16\n",
    "    n_keys = 16\n",
    "    D = 16\n",
    "    q = torch.randn(batch_size, n_queries, D, device=device, requires_grad=True)\n",
    "    k = torch.randn(batch_size, n_keys, D, device=device, requires_grad=True)\n",
    "    v = torch.randn(batch_size, n_keys, D, device=device, requires_grad=True)\n",
    "    do = torch.randn(batch_size, n_queries, D, device=device)\n",
    "\n",
    "    return q, k, v, do\n",
    "\n",
    "q, k, v, do = _make_attn_inputs(device)\n",
    "o_ref, l_ref = _attention_and_lse(q, k, v, is_causal)\n",
    "o = impl(q, k, v, is_causal)\n",
    "# Q, K, V, _do = _make_attn_inputs(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530894b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e42e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad_fn.saved_tensors[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = q.device\n",
    "batch_size, N_q, d = q.shape\n",
    "_, N_k, _ = k.shape\n",
    "B_q = 16\n",
    "B_k = 16\n",
    "O = torch.zeros_like(q, device=device)\n",
    "L = torch.zeros(batch_size, N_q, device=device)\n",
    "for i in range(0, N_q, B_q):\n",
    "    Q_i = q[:, i:i+B_q, :]\n",
    "    O_i = torch.zeros_like(Q_i, device=device)\n",
    "    l_i = torch.zeros(batch_size, B_q, device=device)\n",
    "    m_i = torch.full((batch_size, B_q), float('-inf'), device=device)\n",
    "    for j in range(0, N_k, B_k):\n",
    "        K_j = k[:, j:j+B_k, :]\n",
    "        V_j = v[:, j:j+B_k, :]\n",
    "        S_j = 1 / d**(0.5) * einsum(Q_i, K_j, '... B_q d, ... B_k d -> ... B_q B_k')\n",
    "        assert S_j.shape == (batch_size, B_q, B_k)\n",
    "\n",
    "        m_curr = torch.max(torch.cat([m_i[:, :, None], S_j], axis=-1), axis=-1).values\n",
    "        P_i = torch.exp(S_j - m_curr[:, :, None])\n",
    "        assert P_i.shape == (batch_size, B_q, B_k)\n",
    "\n",
    "        l_i = torch.exp(m_i - m_curr)*l_i + torch.sum(P_i, axis=-1)\n",
    "        # print(l_i)\n",
    "        \n",
    "        _ = torch.diag_embed(torch.exp(m_i - m_curr))\n",
    "        O_i = einsum(_, O_i, '... B_q B_q, ... B_q d -> ... B_q d') + einsum(P_i, V_j, '... B_q B_k, ... B_k d -> ... B_q d')\n",
    "        assert O_i.shape == (batch_size, B_q, d)\n",
    "\n",
    "        m_i = m_curr\n",
    "    O_i = einsum(torch.diag_embed(1 / l_i), O_i, '... B_q B_q, ... B_q d -> ... B_q d')\n",
    "    L_i = m_i + torch.log(l_i)\n",
    "\n",
    "    O[:, i:i+B_q] += O_i\n",
    "    L[:, i:i+B_q] += L_i\n",
    "    print(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed334e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad_fn.saved_tensors[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe323a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad_fn.saved_tensors[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3746a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34141c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract L from the saved tensors\n",
    "assert o.grad_fn.saved_tensors is not None, \"No saved tensors found in the output tensor. Make sure your autograd forward is saving them using ctx.save_for_backward.\"\n",
    "maybe_ls = [t for t in o.grad_fn.saved_tensors if t.shape == (q.shape[0], q.shape[1])]\n",
    "\n",
    "assert len(maybe_ls) == 1, f\"Expected one tensor of shape {q.shape[0], q.shape[1]} in saved tensors, but found {len(maybe_ls)}. The tests require you to save exactly one tensor of this shape, corresponding to the log-sum-exp of the attention scores.\"\n",
    "l = maybe_ls[0]\n",
    "\n",
    "o_ref, l_ref = _attention_and_lse(q, k, v, is_causal)\n",
    "\n",
    "torch.testing.assert_close(o, o_ref, rtol=1e-2, atol=1e-2)\n",
    "torch.testing.assert_close(l, l_ref, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ddced",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = torch.arange(0, 4)\n",
    "col_idx = torch.arange(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb6c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_idx[None, :] > row_idx[:, None]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
