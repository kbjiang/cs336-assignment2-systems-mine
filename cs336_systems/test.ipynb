{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d9499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m\n",
      "triton.runtime.autotuner.Autotuner(\n",
      "    fn,\n",
      "    arg_names,\n",
      "    configs,\n",
      "    key,\n",
      "    reset_to_zero,\n",
      "    restore_value,\n",
      "    pre_hook=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    post_hook=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    prune_configs_by: \u001b[33m'Dict'\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    warmup=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    rep=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    use_cuda_graph=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    do_bench=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "Abstract base class for generic types.\n",
      "\n",
      "A generic type is typically declared by inheriting from\n",
      "this class parameterized with one or more type variables.\n",
      "For example, a generic mapping type might be defined as::\n",
      "\n",
      "  class Mapping(Generic[KT, VT]):\n",
      "      def __getitem__(self, key: KT) -> VT:\n",
      "          ...\n",
      "      # Etc.\n",
      "\n",
      "This class can then be used as follows::\n",
      "\n",
      "  def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n",
      "      try:\n",
      "          return mapping[key]\n",
      "      except KeyError:\n",
      "          return default\n",
      "\u001b[31mSource:\u001b[39m        \n",
      "\u001b[38;5;28;01mclass\u001b[39;00m Autotuner(KernelInterface):\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(\n",
      "        self,\n",
      "        fn,\n",
      "        arg_names,\n",
      "        configs,\n",
      "        key,\n",
      "        reset_to_zero,\n",
      "        restore_value,\n",
      "        pre_hook=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        post_hook=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        prune_configs_by: Dict = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        warmup=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        rep=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "        use_cuda_graph=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "        do_bench=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ):\n",
      "        \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[33m        :param prune_configs_by: a dict of functions that are used to prune configs, fields:\u001b[39m\n",
      "\u001b[33m            'perf_model': performance model used to predicate running time with different configs, returns running time\u001b[39m\n",
      "\u001b[33m            'top_k': number of configs to bench\u001b[39m\n",
      "\u001b[33m            'prune_num_stages_by'(optional): a function used to prune num_stages. It takes configs:List[Config] as its input, and returns pruned configs.\u001b[39m\n",
      "\u001b[33m        \"\"\"\u001b[39m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m configs:\n",
      "            self.configs = [\n",
      "                Config({}, num_warps=\u001b[32m4\u001b[39m, num_stages=\u001b[32m2\u001b[39m, num_ctas=\u001b[32m1\u001b[39m, num_buffers_warp_spec=\u001b[32m0\u001b[39m, num_consumer_groups=\u001b[32m0\u001b[39m,\n",
      "                       reg_dec_producer=\u001b[32m0\u001b[39m, reg_inc_consumer=\u001b[32m0\u001b[39m)\n",
      "            ]\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            self.configs = configs\n",
      "        self.keys = key\n",
      "        self.cache = {}\n",
      "        self.arg_names = arg_names\n",
      "\n",
      "        \u001b[38;5;66;03m# Reset to zero or restore values\u001b[39;00m\n",
      "        self.reset_to_zero = []\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m reset_to_zero \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self.reset_to_zero = list(reset_to_zero)\n",
      "        self.restore_value = []\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m restore_value \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self.restore_value = list(restore_value)\n",
      "\n",
      "        \u001b[38;5;66;03m# Hook to reset or restore for required tensors\u001b[39;00m\n",
      "        self.pre_hook = \u001b[38;5;28;01mlambda\u001b[39;00m kwargs, reset_only=\u001b[38;5;28;01mFalse\u001b[39;00m: \u001b[32m0\u001b[39m\n",
      "        self.post_hook = \u001b[38;5;28;01mlambda\u001b[39;00m kwargs, exception: \u001b[32m0\u001b[39m\n",
      "        self.user_defined_pre_hook = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "        self.user_defined_post_hook = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m pre_hook:\n",
      "            self.pre_hook = pre_hook\n",
      "            self.user_defined_pre_hook = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m (len(self.reset_to_zero) > \u001b[32m0\u001b[39m \u001b[38;5;28;01mor\u001b[39;00m len(self.restore_value) > \u001b[32m0\u001b[39m):\n",
      "\n",
      "            \u001b[38;5;28;01mdef\u001b[39;00m _pre_hook(kwargs, reset_only=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "                \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self.reset_to_zero:\n",
      "                    kwargs[name].zero_()\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m reset_only:\n",
      "                    self.restore_copies = {name: kwargs[name].clone() \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self.restore_value}\n",
      "\n",
      "            self.pre_hook = _pre_hook\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m post_hook:\n",
      "            self.post_hook = post_hook\n",
      "            self.user_defined_post_hook = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "        \u001b[38;5;28;01melif\u001b[39;00m len(self.restore_value) > \u001b[32m0\u001b[39m:\n",
      "\n",
      "            \u001b[38;5;28;01mdef\u001b[39;00m _post_hook(kwargs, exception):\n",
      "                \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;28;01min\u001b[39;00m self.restore_value:\n",
      "                    kwargs[name].copy_(self.restore_copies[name])\n",
      "                self.restore_copies = {}\n",
      "\n",
      "            self.post_hook = _post_hook\n",
      "\n",
      "        self.perf_model = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        self.configs_top_k = \u001b[32m1.0\u001b[39m\n",
      "        self.early_config_prune = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m prune_configs_by:\n",
      "            self.perf_model = prune_configs_by.get(\u001b[33m\"perf_model\"\u001b[39m, self.perf_model)\n",
      "            self.configs_top_k = prune_configs_by.get(\u001b[33m\"top_k\"\u001b[39m, self.configs_top_k)\n",
      "            self.early_config_prune = prune_configs_by.get(\u001b[33m\"early_config_prune\"\u001b[39m, self.early_config_prune)\n",
      "\n",
      "        self.fn = fn\n",
      "        self.base_fn = fn\n",
      "        \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m inspect.isfunction(self.base_fn):\n",
      "            self.base_fn = self.base_fn.fn\n",
      "\n",
      "        self.num_warmups = warmup\n",
      "        self.num_reps = rep\n",
      "        self.use_cuda_graph = use_cuda_graph\n",
      "\n",
      "        \u001b[38;5;66;03m# If we got explicitly called via the old interface, raise a warning\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# and proceed with the old behavior.\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m warmup \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m rep \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m use_cuda_graph:\n",
      "            \u001b[38;5;28;01mimport\u001b[39;00m warnings\n",
      "            warnings.warn((\u001b[33m\"warmup, rep, and use_cuda_graph parameters are deprecated. See \"\u001b[39m\n",
      "                           \u001b[33m\"https://github.com/triton-lang/triton/pull/4496 for details.\"\u001b[39m), DeprecationWarning,\n",
      "                          stacklevel=\u001b[32m1\u001b[39m)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m use_cuda_graph:\n",
      "                \u001b[38;5;28;01mfrom\u001b[39;00m ..testing \u001b[38;5;28;01mimport\u001b[39;00m do_bench_cudagraph\n",
      "                self.do_bench = \u001b[38;5;28;01mlambda\u001b[39;00m kernel_call, quantiles: do_bench_cudagraph(\n",
      "                    kernel_call,\n",
      "                    rep=rep \u001b[38;5;28;01mif\u001b[39;00m rep \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m100\u001b[39m,\n",
      "                    quantiles=quantiles,\n",
      "                )\n",
      "                \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\n",
      "            \u001b[38;5;28;01mimport\u001b[39;00m triton.testing\n",
      "            self.do_bench = \u001b[38;5;28;01mlambda\u001b[39;00m kernel_call, quantiles: triton.testing.do_bench(\n",
      "                kernel_call,\n",
      "                warmup=warmup \u001b[38;5;28;01mif\u001b[39;00m warmup \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m25\u001b[39m,\n",
      "                rep=rep \u001b[38;5;28;01mif\u001b[39;00m rep \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m100\u001b[39m,\n",
      "                quantiles=quantiles,\n",
      "            )\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m do_bench \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            self.do_bench = driver.active.get_benchmarker()\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            self.do_bench = do_bench\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m _bench(self, *args, config, **meta):\n",
      "        \u001b[38;5;28;01mfrom\u001b[39;00m ..compiler.errors \u001b[38;5;28;01mimport\u001b[39;00m CompileTimeAssertionFailure\n",
      "\n",
      "        \u001b[38;5;66;03m# check for conflicts, i.e. meta-parameters both provided\u001b[39;00m\n",
      "        \u001b[38;5;66;03m# as kwargs and by the autotuner\u001b[39;00m\n",
      "        conflicts = meta.keys() & config.kwargs.keys()\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m conflicts:\n",
      "            \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33mf\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\u001b[39m\n",
      "                             \u001b[33m\" Make sure that you don't re-define auto-tuned symbols.\"\u001b[39m)\n",
      "        \u001b[38;5;66;03m# augment meta-parameters with tunable ones\u001b[39;00m\n",
      "        current = dict(meta, **config.all_kwargs())\n",
      "        full_nargs = {**self.nargs, **current}\n",
      "\n",
      "        \u001b[38;5;28;01mdef\u001b[39;00m kernel_call():\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m config.pre_hook:\n",
      "                config.pre_hook(full_nargs)\n",
      "            self.pre_hook(full_nargs)\n",
      "            \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "                self.fn.run(\n",
      "                    *args,\n",
      "                    **current,\n",
      "                )\n",
      "            \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "                \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "                    self.post_hook(full_nargs, exception=e)\n",
      "                \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "                    \u001b[38;5;66;03m# Throw exception raised by `self.fn.run`\u001b[39;00m\n",
      "                    \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\n",
      "            self.post_hook(full_nargs, exception=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\n",
      "        \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m self.do_bench(kernel_call, quantiles=(\u001b[32m0.5\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.8\u001b[39m))\n",
      "        \u001b[38;5;28;01mexcept\u001b[39;00m (OutOfResources, CompileTimeAssertionFailure):\n",
      "            \u001b[38;5;28;01mreturn\u001b[39;00m [float(\u001b[33m\"inf\"\u001b[39m), float(\u001b[33m\"inf\"\u001b[39m), float(\u001b[33m\"inf\"\u001b[39m)]\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m run(self, *args, **kwargs):\n",
      "        self.nargs = dict(zip(self.arg_names, args))\n",
      "        used_cached_result = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m len(self.configs) > \u001b[32m1\u001b[39m:\n",
      "            all_args = {**self.nargs, **kwargs}\n",
      "            _args = {k: v \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;28;01min\u001b[39;00m all_args.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;28;01min\u001b[39;00m self.arg_names}\n",
      "            key = [_args[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m self.keys \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01min\u001b[39;00m _args]\n",
      "            \u001b[38;5;28;01mfor\u001b[39;00m _, arg \u001b[38;5;28;01min\u001b[39;00m _args.items():\n",
      "                \u001b[38;5;28;01mif\u001b[39;00m hasattr(arg, \u001b[33m\"dtype\"\u001b[39m):\n",
      "                    key.append(str(arg.dtype))\n",
      "            key = tuple(key)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self.cache:\n",
      "                \u001b[38;5;66;03m# prune configs\u001b[39;00m\n",
      "                used_cached_result = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "                pruned_configs = self.prune_configs(kwargs)\n",
      "                bench_start = time.time()\n",
      "                timings = {config: self._bench(*args, config=config, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;28;01min\u001b[39;00m pruned_configs}\n",
      "                bench_end = time.time()\n",
      "                self.bench_time = bench_end - bench_start\n",
      "                self.cache[key] = builtins.min(timings, key=timings.get)\n",
      "                full_nargs = {**self.nargs, **kwargs, **self.cache[key].all_kwargs()}\n",
      "                self.pre_hook(full_nargs, reset_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "                self.configs_timings = timings\n",
      "            config = self.cache[key]\n",
      "        \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "            config = self.configs[\u001b[32m0\u001b[39m]\n",
      "        self.best_config = config\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m os.getenv(\u001b[33m\"TRITON_PRINT_AUTOTUNING\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[33m\"1\"\u001b[39m \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m used_cached_result:\n",
      "            print(\u001b[33mf\"Triton autotuning for function {self.base_fn.__name__} finished after \"\u001b[39m\n",
      "                  \u001b[33mf\"{self.bench_time:.2f}s; best config selected: {self.best_config};\"\u001b[39m)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m config.pre_hook \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "            full_nargs = {**self.nargs, **kwargs, **config.all_kwargs()}\n",
      "            config.pre_hook(full_nargs)\n",
      "        ret = self.fn.run(\n",
      "            *args,\n",
      "            **kwargs,\n",
      "            **config.all_kwargs(),\n",
      "        )\n",
      "        self.nargs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m prune_configs(self, kwargs):\n",
      "        pruned_configs = self.configs\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.early_config_prune:\n",
      "            pruned_configs = self.early_config_prune(self.configs, self.nargs, **kwargs)\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m self.perf_model:\n",
      "            top_k = self.configs_top_k\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m isinstance(top_k, float) \u001b[38;5;28;01mand\u001b[39;00m top_k <= \u001b[32m1.0\u001b[39m:\n",
      "                top_k = int(len(self.configs) * top_k)\n",
      "            \u001b[38;5;28;01mif\u001b[39;00m len(pruned_configs) > top_k:\n",
      "                est_timing = {\n",
      "                    config: self.perf_model(\n",
      "                        **self.nargs,\n",
      "                        **kwargs,\n",
      "                        **config.all_kwargs(),\n",
      "                    )\n",
      "                    \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;28;01min\u001b[39;00m pruned_configs\n",
      "                }\n",
      "                pruned_configs = sorted(est_timing.keys(), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: est_timing[x])[:top_k]\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m pruned_configs\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m warmup(self, *args, **kwargs):\n",
      "        self.nargs = dict(zip(self.arg_names, args))\n",
      "        ret = []\n",
      "        \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;28;01min\u001b[39;00m self.prune_configs(kwargs):\n",
      "            ret.append(self.fn.warmup(\n",
      "                *args,\n",
      "                **kwargs,\n",
      "                **config.all_kwargs(),\n",
      "            ))\n",
      "        self.nargs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mFile:\u001b[39m           ~/02-fun/cs336-assignment2-systems-mine/.venv/lib/python3.11/site-packages/triton/runtime/autotuner.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "import triton\n",
    "??triton.runtime.autotuner.Autotuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9952cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_attention import flash_backward_results, _attention_and_lse, _make_attn_inputs\n",
    "from cs336_systems.flashattention import FlashAttentionTriton\n",
    "# from cs336_systems.flashattention_compiled import FlashAttentionTritonCompiled\n",
    "from cs336_systems.flashattention_triton_optimized import FlashAttentionTritonOptimized\n",
    "\n",
    "# impl_0 = AttentionAutogradFunctionPytorch.apply\n",
    "# q, k, v, do = _make_attn_inputs(device='cuda')\n",
    "# o_0 = impl_0(q, k, v, True)\n",
    "# o_0.backward(do)\n",
    "# v.grad\n",
    "\n",
    "# import torch\n",
    "# torch.testing.assert_close(o_0, o_1, rtol=1e-2, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ed22b5-6014-4559-9f70-02dbd8dd6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "impl_0 = FlashAttentionTriton.apply\n",
    "q, k, v, do = _make_attn_inputs(device='cuda')\n",
    "o_0 = impl_0(q, k, v, True)\n",
    "o_0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1089e5-c143-4ad9-a625-5bcf9daebe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "impl_1 = FlashAttentionTritoneOptimized.apply\n",
    "q, k, v, do = _make_attn_inputs(device='cuda')\n",
    "o_1 = impl_1(q, k, v, True)\n",
    "o_1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9c0c73-8596-410a-8a82-59ecfe29c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashAttention2:\n",
    "    def __init__(self, impl, B_q=16, B_k=16):\n",
    "        self.impl = impl\n",
    "        self.B_q = B_q\n",
    "        self.B_k = B_k\n",
    "    \n",
    "    def __call__(self, Q, K, V, is_causal=True):\n",
    "        return self.impl.apply(Q, K, V, is_causal, self.B_q, self.B_k)\n",
    "\n",
    "# # Usage:\n",
    "# flash_small = FlashAttentionPytorch(B_q=8, B_k=8)\n",
    "# o = flash_small(q, k, v, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "def test_timing_flash_forward_backward(test, impl, n_heads, d_head, sequence_length, dtype=torch.bfloat16, device='cuda', B_q=16, B_k=16):\n",
    "    q, k, v = torch.randn(\n",
    "        3, n_heads, sequence_length, d_head, device=device, dtype=dtype, requires_grad=True\n",
    "    )\n",
    "    \n",
    "    flash = torch.compile(FlashAttention2(impl, B_q, B_k))\n",
    "    # sanity check; it would fail without compiling if precision in triton is not implemented right\n",
    "    # flash = FlashAttention2(impl, B_q, B_k)\n",
    "    \n",
    "    def flash_forward():\n",
    "        o = flash(q, k, v, True)\n",
    "\n",
    "    def flash_forward_backward():\n",
    "        o = flash(q, k, v, True)\n",
    "        loss = o.sum()\n",
    "        loss.backward()\n",
    "\n",
    "    if test == \"forward\":\n",
    "        results = triton.testing.do_bench(flash_forward, rep=1000, warmup=1000)\n",
    "    elif test == \"forward_backward\":\n",
    "        results = triton.testing.do_bench(flash_forward_backward, rep=1000, warmup=1000)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong selection.\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075148b-12cf-4643-bbd9-e3bb52cf7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_timing_flash_forward_backward(\"forward_backward\", FlashAttentionAutogradFunctionTriton, 16, 128, 16384, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21573428-41a2-454c-8b3d-324fd2ec2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_timing_flash_forward_backward(\"forward_backward\", FlashAttentionAutogradFunctionTritonBackward, 16, 128, 16384, dtype=torch.bfloat16, B_q=16, B_k=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
